\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{longtable}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\title{Technical Report: MNIST Diffusion Experiments on Purdue Gautschi}
\author{PurdueHPC\_Codex Workflow}
\date{\today}

\begin{document}
\maketitle

\section{Scope and Environment}
This report summarizes end-to-end execution of diffusion model experiments on Purdue RCAC Gautschi, including environment setup, Slurm execution, model details, and resulting artifacts. All commands and scripts were executed under:
\begin{itemize}
  \item Scratch workspace: \texttt{/scratch/gautschi/rmaulik/codex\_test}
  \item Source repository: \texttt{PurdueHPC\_Codex}
  \item Account: \texttt{rmaulik}
  \item Partition: \texttt{ai}
\end{itemize}
No credentials or authentication secrets are stored in this report.

\section{Allocation and Job Accounting}
Current allocation snapshot from \texttt{slist} on Gautschi:
\begin{itemize}
  \item AI partition GPU-hour balance: \textbf{43,798.5}
  \item CPU partition balance: \textbf{0}
\end{itemize}

Completed follow-up experiment jobs (EDM formulation):
\begin{longtable}{l l l r r}
\toprule
Job ID & Name & State & Elapsed (s) & GPUs \\
\midrule
8213207 & mnist\_edm & COMPLETED & 28 & 1 \\
8213218 & mnist\_edm20 & COMPLETED & 30 & 1 \\
\bottomrule
\end{longtable}

Estimated direct GPU-hours from these two runs:
\[
\text{GPU-hours} = \frac{28 + 30}{3600} \times 1 = 0.016 \text{ GPU-hours (approx.)}
\]

\section{Diffusion Model Formulation (EDM)}
The training script now uses an Elucidated Diffusion Model (EDM) style preconditioned denoiser over MNIST ($x_0 \in [-1,1]^{1\times28\times28}$), where the model approximates a score-consistent denoising function across continuous noise levels $\sigma$.

\subsection{EDM Preconditioning and Objective}
Given $x = x_0 + \sigma\epsilon$ with $\epsilon \sim \mathcal{N}(0,I)$ and $\sigma \sim \log\mathcal{N}(p_{\text{mean}}, p_{\text{std}}^2)$, the network is used in preconditioned form:
\[
\hat{x}_0 = c_{\text{skip}}(\sigma)x + c_{\text{out}}(\sigma)F_\theta(c_{\text{in}}(\sigma)x, c_{\text{noise}}(\sigma)),
\]
where
\[
c_{\text{in}}=\frac{1}{\sqrt{\sigma^2+\sigma_{\text{data}}^2}},\quad
c_{\text{skip}}=\frac{\sigma_{\text{data}}^2}{\sigma^2+\sigma_{\text{data}}^2},\quad
c_{\text{out}}=\frac{\sigma\sigma_{\text{data}}}{\sqrt{\sigma^2+\sigma_{\text{data}}^2}}.
\]
Training minimizes weighted denoising MSE:
\[
\mathcal{L}(\theta)=\mathbb{E}_{x_0,\sigma,\epsilon}\left[w(\sigma)\|\hat{x}_0 - x_0\|_2^2\right],\quad
w(\sigma)=\frac{\sigma^2+\sigma_{\text{data}}^2}{(\sigma\sigma_{\text{data}})^2}.
\]

\subsection{EDM Sampling}
Sampling follows a Karras $\sigma$-schedule from $\sigma_{\max}$ to $\sigma_{\min}$ (then 0) and integrates the probability flow ODE in $\sigma$-space:
\[
\frac{dx}{d\sigma}=\frac{x-\hat{x}_0(x,\sigma)}{\sigma},
\]
with Euler step plus second-order (Heun) correction.

\section{Network Architecture}
The current model is a residual U-Net style denoiser with sinusoidal time embeddings:
\begin{itemize}
  \item Stem: Conv $1\rightarrow64$
  \item Down path: residual blocks at 64 and 128 channels with strided downsampling ($28\rightarrow14\rightarrow7$)
  \item Bottleneck: two residual blocks at 256 channels
  \item Up path: transposed convolutions + skip concatenation + residual blocks ($7\rightarrow14\rightarrow28$)
  \item Output: GroupNorm + Conv $64\rightarrow1$
  \item Optimizer: AdamW, gradient clipping (max norm 1.0)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{report/assets/gautschi_edm_8213207/architecture_schematic.png}
\caption{U-Net denoiser schematic generated by the EDM training pipeline.}
\end{figure}

\section{Latest EDM Run Configuration}
The latest EDM follow-up run (job \texttt{8213207}) used:
\begin{itemize}
  \item Epochs: 1
  \item Karras sampling steps: 80
  \item Batch size: 128
  \item EDM parameters: $\sigma_{\min}=0.002$, $\sigma_{\max}=80.0$, $\rho=7.0$, $\sigma_{\text{data}}=0.5$
  \item Sigma training distribution: $p_{\text{mean}}=-1.2$, $p_{\text{std}}=1.2$
  \item Slurm resources: 1 GPU, 14 CPUs, partition \texttt{ai}
\end{itemize}

From \texttt{metrics.json} (run \texttt{edm\_dps\_70pct\_8213207}):
\begin{itemize}
  \item Total steps: 469
  \item Final epoch mean loss: 0.273500
  \item Best epoch mean loss: 0.273500
  \item Device: CUDA
\end{itemize}

\section{Results and Artifacts}
\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{report/assets/gautschi_edm_8213207/mnist_samples.png}
\caption{Generated MNIST samples from the EDM run (\texttt{8213207}).}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_edm_8213207/loss_curve_step.png}
\caption{Per-step training loss trajectory for EDM run \texttt{8213207}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.88\textwidth]{report/assets/gautschi_edm_8213207/loss_curve_epoch.png}
\caption{Per-epoch mean loss trajectory for EDM run \texttt{8213207}.}
\end{figure}

\section{Posterior Sampling with Partial Observations}
We perform diffusion posterior sampling in EDM sigma-space. Let $y$ denote observed pixels, $m \in \{0,1\}^{H\times W}$ the mask, and $\hat{x}_0(x,\sigma)$ the EDM denoised estimate.

\subsection*{Mathematical Formulation}
Posterior guidance is applied through a masked Gaussian likelihood:
\[
p(y \mid x_0) \propto \exp\left(-\frac{\|m\odot (y-x_0)\|_2^2}{2\sigma_t^2}\right), \quad
\sigma_t^2=\sigma_y^2 + c\sigma^2.
\]
This yields the guidance gradient (used in code):
\[
g_\sigma \approx \frac{m\odot (y-\hat{x}_0)}{\sigma_t^2}.
\]

In EDM ODE form, we use:
\[
\frac{dx}{d\sigma} \approx \frac{x - (\hat{x}_0 + \lambda_\sigma \sigma g_\sigma)}{\sigma},
\]
with timestep-annealed guidance strength
\[
\lambda_\sigma=\lambda_{\max}\left(\lambda_{\min} + (1-\lambda_{\min})\left(\frac{i}{N-1}\right)^p\right).
\]

We additionally enforce projection-based data consistency:
\[
\hat{x}_0 \leftarrow m\odot y + (1-m)\odot \hat{x}_0,
\]
and during intermediate sigma steps:
\[
x \leftarrow m\odot (y + \sigma \xi) + (1-m)\odot x.
\]

The implemented strategy therefore combines:
\begin{itemize}
  \item Noise-aware likelihood variance: $\sigma_t^2 = \sigma_y^2 + c\sigma^2$
  \item Annealed guidance schedule: weak guidance at high noise, stronger guidance near final denoise steps
  \item Data-consistency projection on observed pixels (in both $\hat{x}_0$ and sampled states)
\end{itemize}

\subsection*{Experiment A: 70\% Pixels Revealed}
\begin{itemize}
  \item Observed fraction: 70\% of pixels (30\% occluded, random mask)
  \item Target digit class: 7
  \item Guidance scale: 1.5
  \item Guidance min fraction / power: 0.25 / 1.5
  \item Likelihood noise scale: 0.1
  \item Noise-aware coefficient: 0.05
\end{itemize}

Validated Gautschi posterior job:
\begin{itemize}
  \item Job ID: \texttt{8213207}
  \item State: \texttt{COMPLETED}, exit code \texttt{0:0}
  \item Elapsed: \texttt{00:00:28}
  \item Run tag: \texttt{edm\_dps\_70pct\_8213207}
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_edm_8213207/posterior_conditioning_overview.png}
\caption{EDM posterior conditioning setup: ground truth, observed mask (70\% observed), observed pixels, and posterior draws.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{report/assets/gautschi_edm_8213207/posterior_samples.png}
\caption{EDM posterior samples with noise-aware likelihood, guidance annealing, and projection-based data consistency (70\% revealed).}
\end{figure}

\subsection*{Experiment B: 20\% Pixels Revealed}
\begin{itemize}
  \item Observed fraction: 20\% of pixels (80\% occluded, random mask)
  \item Target digit class: 7
  \item Guidance scale: 1.5
  \item Guidance min fraction / power: 0.25 / 1.5
  \item Likelihood noise scale: 0.1
  \item Noise-aware coefficient: 0.05
\end{itemize}

Validated Gautschi posterior job:
\begin{itemize}
  \item Job ID: \texttt{8213218}
  \item State: \texttt{COMPLETED}, exit code \texttt{0:0}
  \item Elapsed: \texttt{00:00:30}
  \item Run tag: \texttt{edm\_dps\_20pct\_8213218}
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_edm_8213218/posterior_conditioning_overview.png}
\caption{EDM posterior conditioning setup: ground truth, observed mask (20\% observed), observed pixels, and posterior draws.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{report/assets/gautschi_edm_8213218/posterior_samples.png}
\caption{EDM posterior samples with noise-aware likelihood, guidance annealing, and projection-based data consistency (20\% revealed).}
\end{figure}

\section{Intrinsic Dimensionality of Posterior Samples}
To analyze posterior sample complexity, we generated a large posterior ensemble for each setting and formed a snapshot matrix:
\[
X = [x^{(1)}, x^{(2)}, \ldots, x^{(N)}] \in \mathbb{R}^{784 \times N},
\]
where each column is a flattened $28\times28$ posterior sample and $N=1024 > 784$.
We computed
\[
X = U\Sigma V^\top,
\]
and inspected the singular value spectrum $\{\sigma_i\}$ and cumulative energy.

SVD jobs on Gautschi:
\begin{itemize}
  \item 70\% observed: job \texttt{8213341}, run \texttt{edm\_svd\_70pct\_8213341}
  \item 20\% observed: job \texttt{8213342}, run \texttt{edm\_svd\_20pct\_8213342}
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{lrrrrr}
\toprule
Observed fraction & Pixels & Snapshots & $k_{95}$ & $k_{99}$ & Participation ratio \\
\midrule
70\% & 784 & 1024 & 1 & 3 & 1.023 \\
20\% & 784 & 1024 & 45 & 152 & 1.718 \\
\bottomrule
\end{tabular}
\caption{Singular-spectrum summary from posterior snapshot matrices.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.82\textwidth]{report/assets/gautschi_edm_svd_70pct_8213341/singular_value_spectrum.png}
\caption{Normalized singular value spectrum for EDM posterior samples with 70\% observed pixels.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.82\textwidth]{report/assets/gautschi_edm_svd_20pct_8213342/singular_value_spectrum.png}
\caption{Normalized singular value spectrum for EDM posterior samples with 20\% observed pixels.}
\end{figure}

\subsection*{Additional Subsection: Score-Snapshot SVD and Spectrum Overlay}
We repeated the intrinsic dimensionality analysis by constructing snapshots from the \emph{estimated score field} instead of the samples. For each posterior sample $x^{(j)}$, we evaluated the approximated score at fixed $\sigma_{\text{eval}}=0.1$:
\[
s^{(j)} \approx \frac{\hat{x}_0(x^{(j)}, \sigma_{\text{eval}}) - x^{(j)}}{\sigma_{\text{eval}}^2},
\]
then formed
\[
S = [s^{(1)}, s^{(2)}, \ldots, s^{(N)}] \in \mathbb{R}^{784 \times N},
\]
with $N=1024 > 784$, and computed its SVD.

Score-snapshot jobs on Gautschi:
\begin{itemize}
  \item 70\% observed: job \texttt{8213437}, run \texttt{edm\_score\_svd\_70pct\_8213437}
  \item 20\% observed: job \texttt{8213438}, run \texttt{edm\_score\_svd\_20pct\_8213438}
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{lrrrr}
\toprule
Observed fraction & Snapshot type & $k_{95}$ & $k_{99}$ & Participation ratio \\
\midrule
70\% & Sample (previous) & 1 & 2 & 1.023 \\
70\% & Score (current) & 63 & 137 & 2.566 \\
20\% & Sample (previous) & 46 & 154 & 1.735 \\
20\% & Score (current) & 293 & 464 & 8.786 \\
\bottomrule
\end{tabular}
\caption{Comparison of singular-spectrum intrinsic dimensionality metrics for sample- vs score-based snapshots.}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{report/assets/gautschi_edm_score_svd_70pct_8213437/singular_value_spectrum_overlay.png}
\caption{Overlay of normalized singular value spectra (70\% observed): previous sample-snapshot experiment vs current score-snapshot experiment.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{report/assets/gautschi_edm_score_svd_20pct_8213438/singular_value_spectrum_overlay.png}
\caption{Overlay of normalized singular value spectra (20\% observed): previous sample-snapshot experiment vs current score-snapshot experiment.}
\end{figure}

\section{Live Dashboard Operation}
The workflow now maintains a root live dashboard at:
\begin{itemize}
  \item \texttt{outputs/dashboard.html}
\end{itemize}
This dashboard follows the latest run via:
\begin{itemize}
  \item \texttt{outputs/LATEST\_RUN.txt}
  \item \texttt{outputs/current} symlink
\end{itemize}
and supports:
\begin{itemize}
  \item periodic refresh during active runs,
  \item automatic stop of refresh when status reaches \texttt{completed},
  \item image zoom controls for plots/schematics.
\end{itemize}

\section{Reproducibility Notes}
Primary scripts used for these experiments:
\begin{itemize}
  \item \texttt{mnist\_diffusion.py}
  \item \texttt{posterior\_svd\_analysis.py}
  \item \texttt{posterior\_score\_svd\_analysis.py}
  \item \texttt{submit\_mnist\_edm\_posterior.slurm}
  \item \texttt{submit\_mnist\_edm\_posterior\_20pct.slurm}
  \item \texttt{serve\_dashboard.sh}
\end{itemize}

\subsection*{Persistent SSH Connection Reuse}
To avoid repeated password + 2FA prompts for each \texttt{ssh}/\texttt{scp} call, use SSH multiplexing with a persistent control socket:
\begin{verbatim}
ssh -M -S /tmp/gautschi_mux.sock -o ControlPersist=8h -N \
  rmaulik@gautschi.rcac.purdue.edu
\end{verbatim}

Then reuse that authenticated channel:
\begin{verbatim}
ssh -S /tmp/gautschi_mux.sock rmaulik@gautschi.rcac.purdue.edu
scp -o ControlMaster=auto -o ControlPath=/tmp/gautschi_mux.sock \
  <src> <dst>
\end{verbatim}

Check and close the master session:
\begin{verbatim}
ssh -S /tmp/gautschi_mux.sock -O check rmaulik@gautschi.rcac.purdue.edu
ssh -S /tmp/gautschi_mux.sock -O exit  rmaulik@gautschi.rcac.purdue.edu
\end{verbatim}

To regenerate this report PDF locally:
\begin{verbatim}
cd PurdueHPC_Codex/report
latexmk -pdf -interaction=nonstopmode -halt-on-error \
  -output-directory=../output/pdf gautschi_diffusion_report.tex
\end{verbatim}

\end{document}
