\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{longtable}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\title{Technical Report: MNIST Diffusion Experiments on Purdue Gautschi}
\author{PurdueHPC\_Codex Workflow}
\date{\today}

\begin{document}
\maketitle

\section{Scope and Environment}
This report summarizes end-to-end execution of diffusion model experiments on Purdue RCAC Gautschi, including environment setup, Slurm execution, model details, and resulting artifacts. All commands and scripts were executed under:
\begin{itemize}
  \item Scratch workspace: \texttt{/scratch/gautschi/rmaulik/codex\_test}
  \item Source repository: \texttt{PurdueHPC\_Codex}
  \item Account: \texttt{rmaulik}
  \item Partition: \texttt{ai}
\end{itemize}
No credentials or authentication secrets are stored in this report.

\section{Allocation and Job Accounting}
Current allocation snapshot from \texttt{slist} on Gautschi:
\begin{itemize}
  \item AI partition GPU-hour balance: \textbf{43,795.8}
  \item CPU partition balance: \textbf{0}
\end{itemize}

Completed follow-up experiment jobs (EDM + intrinsic-dimension studies):
\begin{longtable}{l l l r r}
\toprule
Job ID & Name & State & Elapsed (s) & GPUs \\
\midrule
8213207 & mnist\_edm & COMPLETED & 28 & 1 \\
8213218 & mnist\_edm20 & COMPLETED & 30 & 1 \\
8213341 & svd70 & COMPLETED & 32 & 1 \\
8213342 & svd20 & COMPLETED & 32 & 1 \\
8213437 & ssvd70 & COMPLETED & 27 & 1 \\
8213438 & ssvd20 & COMPLETED & 27 & 1 \\
8213702 & gid70 & COMPLETED & 30 & 1 \\
8213703 & gid20 & COMPLETED & 30 & 1 \\
8214018 & psvd & COMPLETED & 27 & 1 \\
8214200 & rsvd20 & COMPLETED & 36 & 1 \\
8214779 & rsvd0 & COMPLETED & 43 & 1 \\
\bottomrule
\end{longtable}

Estimated direct GPU-hours from these accounted runs:
\[
\text{GPU-hours} = \frac{342}{3600} \times 1 = 0.095 \text{ GPU-hours (approx.)}
\]

\section{Diffusion Model Formulation (EDM)}
The training script now uses an Elucidated Diffusion Model (EDM) style preconditioned denoiser over MNIST ($x_0 \in [-1,1]^{1\times28\times28}$), where the model approximates a score-consistent denoising function across continuous noise levels $\sigma$.

\subsection{EDM Preconditioning and Objective}
Given $x = x_0 + \sigma\epsilon$ with $\epsilon \sim \mathcal{N}(0,I)$ and $\sigma \sim \log\mathcal{N}(p_{\text{mean}}, p_{\text{std}}^2)$, the network is used in preconditioned form:
\[
\hat{x}_0 = c_{\text{skip}}(\sigma)x + c_{\text{out}}(\sigma)F_\theta(c_{\text{in}}(\sigma)x, c_{\text{noise}}(\sigma)),
\]
where
\[
c_{\text{in}}=\frac{1}{\sqrt{\sigma^2+\sigma_{\text{data}}^2}},\quad
c_{\text{skip}}=\frac{\sigma_{\text{data}}^2}{\sigma^2+\sigma_{\text{data}}^2},\quad
c_{\text{out}}=\frac{\sigma\sigma_{\text{data}}}{\sqrt{\sigma^2+\sigma_{\text{data}}^2}}.
\]
Training minimizes weighted denoising MSE:
\[
\mathcal{L}(\theta)=\mathbb{E}_{x_0,\sigma,\epsilon}\left[w(\sigma)\|\hat{x}_0 - x_0\|_2^2\right],\quad
w(\sigma)=\frac{\sigma^2+\sigma_{\text{data}}^2}{(\sigma\sigma_{\text{data}})^2}.
\]

\subsection{EDM Sampling}
Sampling follows a Karras $\sigma$-schedule from $\sigma_{\max}$ to $\sigma_{\min}$ (then 0) and integrates the probability flow ODE in $\sigma$-space:
\[
\frac{dx}{d\sigma}=\frac{x-\hat{x}_0(x,\sigma)}{\sigma},
\]
with Euler step plus second-order (Heun) correction.

\section{Network Architecture}
The current model is a residual U-Net style denoiser with sinusoidal time embeddings:
\begin{itemize}
  \item Stem: Conv $1\rightarrow64$
  \item Down path: residual blocks at 64 and 128 channels with strided downsampling ($28\rightarrow14\rightarrow7$)
  \item Bottleneck: two residual blocks at 256 channels
  \item Up path: transposed convolutions + skip concatenation + residual blocks ($7\rightarrow14\rightarrow28$)
  \item Output: GroupNorm + Conv $64\rightarrow1$
  \item Optimizer: AdamW, gradient clipping (max norm 1.0)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{report/assets/gautschi_edm_8213207/architecture_schematic.png}
\caption{U-Net denoiser schematic generated by the EDM training pipeline.}
\end{figure}

\section{Latest EDM Run Configuration}
The latest EDM follow-up run (job \texttt{8213207}) used:
\begin{itemize}
  \item Epochs: 1
  \item Karras sampling steps: 80
  \item Batch size: 128
  \item EDM parameters: $\sigma_{\min}=0.002$, $\sigma_{\max}=80.0$, $\rho=7.0$, $\sigma_{\text{data}}=0.5$
  \item Sigma training distribution: $p_{\text{mean}}=-1.2$, $p_{\text{std}}=1.2$
  \item Slurm resources: 1 GPU, 14 CPUs, partition \texttt{ai}
\end{itemize}

From \texttt{metrics.json} (run \texttt{edm\_dps\_70pct\_8213207}):
\begin{itemize}
  \item Total steps: 469
  \item Final epoch mean loss: 0.273500
  \item Best epoch mean loss: 0.273500
  \item Device: CUDA
\end{itemize}

\section{Results and Artifacts}
\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{report/assets/gautschi_edm_8213207/mnist_samples.png}
\caption{Generated MNIST samples from the EDM run (\texttt{8213207}).}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_edm_8213207/loss_curve_step.png}
\caption{Per-step training loss trajectory for EDM run \texttt{8213207}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.88\textwidth]{report/assets/gautschi_edm_8213207/loss_curve_epoch.png}
\caption{Per-epoch mean loss trajectory for EDM run \texttt{8213207}.}
\end{figure}

\section{Posterior Sampling with Partial Observations}
We perform diffusion posterior sampling in EDM sigma-space. Let $y$ denote observed pixels, $m \in \{0,1\}^{H\times W}$ the mask, and $\hat{x}_0(x,\sigma)$ the EDM denoised estimate.

\subsection*{Mathematical Formulation}
Posterior guidance is applied through a masked Gaussian likelihood:
\[
p(y \mid x_0) \propto \exp\left(-\frac{\|m\odot (y-x_0)\|_2^2}{2\sigma_t^2}\right), \quad
\sigma_t^2=\sigma_y^2 + c\sigma^2.
\]
This yields the guidance gradient (used in code):
\[
g_\sigma \approx \frac{m\odot (y-\hat{x}_0)}{\sigma_t^2}.
\]

In EDM ODE form, we use:
\[
\frac{dx}{d\sigma} \approx \frac{x - (\hat{x}_0 + \lambda_\sigma \sigma g_\sigma)}{\sigma},
\]
with timestep-annealed guidance strength
\[
\lambda_\sigma=\lambda_{\max}\left(\lambda_{\min} + (1-\lambda_{\min})\left(\frac{i}{N-1}\right)^p\right).
\]

We additionally enforce projection-based data consistency:
\[
\hat{x}_0 \leftarrow m\odot y + (1-m)\odot \hat{x}_0,
\]
and during intermediate sigma steps:
\[
x \leftarrow m\odot (y + \sigma \xi) + (1-m)\odot x.
\]

The implemented strategy therefore combines:
\begin{itemize}
  \item Noise-aware likelihood variance: $\sigma_t^2 = \sigma_y^2 + c\sigma^2$
  \item Annealed guidance schedule: weak guidance at high noise, stronger guidance near final denoise steps
  \item Data-consistency projection on observed pixels (in both $\hat{x}_0$ and sampled states)
\end{itemize}

\subsection*{Experiment A: 70\% Pixels Revealed}
\begin{itemize}
  \item Observed fraction: 70\% of pixels (30\% occluded, random mask)
  \item Target digit class: 7
  \item Guidance scale: 1.5
  \item Guidance min fraction / power: 0.25 / 1.5
  \item Likelihood noise scale: 0.1
  \item Noise-aware coefficient: 0.05
\end{itemize}

Validated Gautschi posterior job:
\begin{itemize}
  \item Job ID: \texttt{8213207}
  \item State: \texttt{COMPLETED}, exit code \texttt{0:0}
  \item Elapsed: \texttt{00:00:28}
  \item Run tag: \texttt{edm\_dps\_70pct\_8213207}
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_edm_8213207/posterior_conditioning_overview.png}
\caption{EDM posterior conditioning setup: ground truth, observed mask (70\% observed), observed pixels, and posterior draws.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{report/assets/gautschi_edm_8213207/posterior_samples.png}
\caption{EDM posterior samples with noise-aware likelihood, guidance annealing, and projection-based data consistency (70\% revealed).}
\end{figure}

\subsection*{Experiment B: 20\% Pixels Revealed}
\begin{itemize}
  \item Observed fraction: 20\% of pixels (80\% occluded, random mask)
  \item Target digit class: 7
  \item Guidance scale: 1.5
  \item Guidance min fraction / power: 0.25 / 1.5
  \item Likelihood noise scale: 0.1
  \item Noise-aware coefficient: 0.05
\end{itemize}

Validated Gautschi posterior job:
\begin{itemize}
  \item Job ID: \texttt{8213218}
  \item State: \texttt{COMPLETED}, exit code \texttt{0:0}
  \item Elapsed: \texttt{00:00:30}
  \item Run tag: \texttt{edm\_dps\_20pct\_8213218}
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_edm_8213218/posterior_conditioning_overview.png}
\caption{EDM posterior conditioning setup: ground truth, observed mask (20\% observed), observed pixels, and posterior draws.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{report/assets/gautschi_edm_8213218/posterior_samples.png}
\caption{EDM posterior samples with noise-aware likelihood, guidance annealing, and projection-based data consistency (20\% revealed).}
\end{figure}

\section{Intrinsic Dimensionality of Posterior Samples}
To analyze posterior sample complexity, we generated a large posterior ensemble for each setting and formed a snapshot matrix:
\[
X = [x^{(1)}, x^{(2)}, \ldots, x^{(N)}] \in \mathbb{R}^{784 \times N},
\]
where each column is a flattened $28\times28$ posterior sample and $N=1024 > 784$.
We computed
\[
X = U\Sigma V^\top,
\]
and inspected the singular value spectrum $\{\sigma_i\}$ and cumulative energy.
For this SVD-based analysis, we report
\[
d_{95}=\min\left\{k:\frac{\sum_{i=1}^{k}\sigma_i^2}{\sum_{j}\sigma_j^2}\ge 0.95\right\},\qquad
d_{99}=\min\left\{k:\frac{\sum_{i=1}^{k}\sigma_i^2}{\sum_{j}\sigma_j^2}\ge 0.99\right\}.
\]

SVD jobs on Gautschi:
\begin{itemize}
  \item 70\% observed: job \texttt{8213341}, run \texttt{edm\_svd\_70pct\_8213341}
  \item 20\% observed: job \texttt{8213342}, run \texttt{edm\_svd\_20pct\_8213342}
  \item Prior (unconditional): job \texttt{8214018}, run \texttt{edm\_prior\_svd\_8214018}
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{lrrrrr}
\toprule
Case & Pixels & Snapshots & $d_{95}$ & $d_{99}$ & Participation ratio \\
\midrule
70\% & 784 & 1024 & 1 & 3 & 1.023 \\
20\% & 784 & 1024 & 45 & 152 & 1.718 \\
Prior (unconditional) & 784 & 1024 & 47 & 158 & 1.812 \\
\bottomrule
\end{tabular}
\caption{Singular-spectrum summary from sample snapshot matrices (posterior-conditioned and prior-unconditional).}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{report/assets/gautschi_edm_svd_combined/singular_value_spectrum_samples_70_vs_20.png}
\caption{Normalized singular value spectrum for sample snapshots: 70\% observed, 20\% observed, and prior-unconditional.}
\end{figure}

\subsection*{Additional Subsection: Score-Snapshot SVD and Combined Spectra}
We repeated the intrinsic dimensionality analysis by constructing snapshots from the \emph{estimated score field} instead of the samples. For each posterior sample $x^{(j)}$, we evaluated the approximated score at fixed $\sigma_{\text{eval}}=0.1$:
\[
s^{(j)} \approx \frac{\hat{x}_0(x^{(j)}, \sigma_{\text{eval}}) - x^{(j)}}{\sigma_{\text{eval}}^2},
\]
then formed
\[
S = [s^{(1)}, s^{(2)}, \ldots, s^{(N)}] \in \mathbb{R}^{784 \times N},
\]
with $N=1024 > 784$, and computed its SVD.

Score-snapshot jobs on Gautschi:
\begin{itemize}
  \item 70\% observed: job \texttt{8213437}, run \texttt{edm\_score\_svd\_70pct\_8213437}
  \item 20\% observed: job \texttt{8213438}, run \texttt{edm\_score\_svd\_20pct\_8213438}
  \item Prior (unconditional): job \texttt{8214018}, run \texttt{edm\_prior\_svd\_8214018}
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{lrrrr}
\toprule
Case & Snapshot type & $d_{95}$ & $d_{99}$ & Participation ratio \\
\midrule
70\% & Sample (previous) & 1 & 2 & 1.023 \\
70\% & Score (current) & 63 & 137 & 2.566 \\
20\% & Sample (previous) & 46 & 154 & 1.735 \\
20\% & Score (current) & 293 & 464 & 8.786 \\
Prior (unconditional) & Sample & 47 & 158 & 1.812 \\
Prior (unconditional) & Score & 374 & 536 & 66.136 \\
\bottomrule
\end{tabular}
\caption{Comparison of singular-spectrum intrinsic-dimensionality metrics for sample- vs score-based snapshots, including prior-only (no posterior sampling).}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{report/assets/gautschi_edm_svd_combined/singular_value_spectrum_scores_70_vs_20.png}
\caption{Normalized singular value spectrum for score snapshots: 70\% observed, 20\% observed, and prior-unconditional.}
\end{figure}

\subsection*{Additional Subsection: Randomized Top-20 Score Spectrum vs Visible Fraction}
We ran an additional score-snapshot experiment for progressively increased visible-pixel fractions:
\[
0\%,\; 20\%,\; 40\%,\; 60\%,\; 80\%.
\]
For each fraction, we formed the score snapshot matrix
\[
S = [s^{(1)}, \ldots, s^{(N)}] \in \mathbb{R}^{784 \times 1024},
\]
and used a randomized SVD (instead of full SVD) to estimate only the leading 20 singular values.
Given random test matrix $\Omega$, we computed
\[
Y = S\Omega,\quad Q=\mathrm{orth}(Y),\quad B=Q^\top S,
\]
then took the leading singular values of $B$ as approximations of the leading singular values of $S$.

Randomized sweep job on Gautschi:
\begin{itemize}
  \item job \texttt{8214779}, run \texttt{edm\_randomized\_score\_svd\_sweep\_with\_prior\_8214779}
\end{itemize}
Because this is a truncated top-20 spectrum estimate, we use it here for comparative spectral shape only (not for intrinsic-dimension indicators).

\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{report/assets/gautschi_edm_randomized_score_svd_sweep_with_prior_8214779/randomized_score_spectrum_top20_overlay.png}
\caption{Randomized top-20 score-snapshot spectrum for visible fractions 0/20/40/60/80\% (0\% is prior-only sampling).}
\end{figure}

\subsection*{Additional Subsection: Sampling-Free Posterior Geometry ID}
To quantify how partial observations change intrinsic dimensionality \emph{without posterior sampling}, we used a local Gaussian approximation around a MAP point.
For each observed mask, we estimated $\hat{x}_{\mathrm{MAP}}$ using score-guided ascent on:
\[
\nabla_x \log p(x \mid y, M)
\approx s_\theta(x,\sigma_{\mathrm{eval}})
\;+\; \frac{M^\top (y - Mx)}{\sigma_y^2},
\]
then formed a local posterior precision proxy:
\[
\Lambda(\hat{x}_{\mathrm{MAP}})
\approx -J_s(\hat{x}_{\mathrm{MAP}}, \sigma_{\mathrm{eval}})
\;+\; \frac{M^\top M}{\sigma_y^2},
\]
where $J_s$ is the Jacobian of the approximated score.
Let the (nonnegative) precision eigenvalues be sorted in descending order:
\[
\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_{784} \ge 0.
\]
We define normalized precision-energy weights
\[
w_i = \frac{\lambda_i}{\sum_{j=1}^{784}\lambda_j},
\]
and cumulative precision energy
\[
E(k) = \sum_{i=1}^{k} w_i.
\]
Then the codimension thresholds are computed as the \emph{smallest} indices whose cumulative precision energy reaches the target level:
\[
c_{95} = \min\{k:\; E(k)\ge 0.95\}, \qquad
c_{99} = \min\{k:\; E(k)\ge 0.99\}.
\]
Finally, we convert codimension to an intrinsic-dimension proxy by subtracting from ambient dimension 784:
\[
d_{95}^{\mathrm{proxy}} = 784 - c_{95}, \qquad
d_{99}^{\mathrm{proxy}} = 784 - c_{99}.
\]

Geometry-ID jobs on Gautschi:
\begin{itemize}
  \item 70\% observed: job \texttt{8213702}, run \texttt{edm\_geometry\_id\_70pct\_8213702}
  \item 20\% observed: job \texttt{8213703}, run \texttt{edm\_geometry\_id\_20pct\_8213703}
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{lrrrr}
\toprule
Observed fraction & $c_{95}$ & $d_{95}^{\mathrm{proxy}}$ & $c_{99}$ & $d_{99}^{\mathrm{proxy}}$ \\
\midrule
70\% & 693 & 91 & 747 & 37 \\
20\% & 654 & 130 & 724 & 60 \\
\bottomrule
\end{tabular}
\caption{Sampling-free posterior-geometry intrinsic-dimension proxies from local precision spectra.}
\end{table}

These sampling-free proxies show the expected trend: with fewer observed pixels (20\%), the posterior is less constrained and has higher intrinsic dimensionality than the 70\% observed case.

\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{report/assets/gautschi_edm_geometry_id_combined/local_covariance_spectrum_70_vs_20.png}
\caption{Sampling-free local covariance spectrum (MAP + score-Jacobian approximation): 70\% observed vs 20\% observed.}
\end{figure}

\section{Live Dashboard Operation}
The workflow now maintains a root live dashboard at:
\begin{itemize}
  \item \texttt{outputs/dashboard.html}
\end{itemize}
This dashboard follows the latest run via:
\begin{itemize}
  \item \texttt{outputs/LATEST\_RUN.txt}
  \item \texttt{outputs/current} symlink
\end{itemize}
and supports:
\begin{itemize}
  \item periodic refresh during active runs,
  \item automatic stop of refresh when status reaches \texttt{completed},
  \item image zoom controls for plots/schematics.
\end{itemize}

\section{Reproducibility Notes}
Primary scripts used for these experiments:
\begin{itemize}
  \item \texttt{mnist\_diffusion.py}
  \item \texttt{posterior\_svd\_analysis.py}
  \item \texttt{posterior\_score\_svd\_analysis.py}
  \item \texttt{posterior\_score\_randomized\_svd\_sweep.py}
  \item \texttt{prior\_score\_svd\_analysis.py}
  \item \texttt{posterior\_geometry\_id\_analysis.py}
  \item \texttt{submit\_mnist\_edm\_posterior.slurm}
  \item \texttt{submit\_mnist\_edm\_posterior\_20pct.slurm}
  \item \texttt{serve\_dashboard.sh}
\end{itemize}

\subsection*{Persistent SSH Connection Reuse}
To avoid repeated password + 2FA prompts for each \texttt{ssh}/\texttt{scp} call, use SSH multiplexing with a persistent control socket:
\begin{verbatim}
ssh -M -S /tmp/gautschi_mux.sock -o ControlPersist=8h -N \
  rmaulik@gautschi.rcac.purdue.edu
\end{verbatim}

Then reuse that authenticated channel:
\begin{verbatim}
ssh -S /tmp/gautschi_mux.sock rmaulik@gautschi.rcac.purdue.edu
scp -o ControlMaster=auto -o ControlPath=/tmp/gautschi_mux.sock \
  <src> <dst>
\end{verbatim}

Check and close the master session:
\begin{verbatim}
ssh -S /tmp/gautschi_mux.sock -O check rmaulik@gautschi.rcac.purdue.edu
ssh -S /tmp/gautschi_mux.sock -O exit  rmaulik@gautschi.rcac.purdue.edu
\end{verbatim}

To regenerate this report PDF locally:
\begin{verbatim}
cd PurdueHPC_Codex/report
latexmk -pdf -interaction=nonstopmode -halt-on-error \
  -output-directory=../output/pdf gautschi_diffusion_report.tex
\end{verbatim}

\end{document}
