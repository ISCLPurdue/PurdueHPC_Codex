\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{longtable}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\title{Technical Report: MNIST Diffusion Experiments on Purdue Gautschi}
\author{PurdueHPC\_Codex Workflow}
\date{\today}

\begin{document}
\maketitle

\section{Scope and Environment}
This report summarizes end-to-end execution of diffusion model experiments on Purdue RCAC Gautschi, including environment setup, Slurm execution, model details, and resulting artifacts. All commands and scripts were executed under:
\begin{itemize}
  \item Scratch workspace: \texttt{/scratch/gautschi/rmaulik/codex\_test}
  \item Source repository: \texttt{PurdueHPC\_Codex}
  \item Account: \texttt{rmaulik}
  \item Partition: \texttt{ai}
\end{itemize}
No credentials or authentication secrets are stored in this report.

\section{Allocation and Job Accounting}
Current allocation snapshot from \texttt{slist} on Gautschi:
\begin{itemize}
  \item AI partition GPU-hour balance: \textbf{43,798.5}
  \item CPU partition balance: \textbf{0}
\end{itemize}

Completed experiment jobs:
\begin{longtable}{l l l r r}
\toprule
Job ID & Name & State & Elapsed (s) & GPUs \\
\midrule
8109439 & mnist\_ddpm & COMPLETED & 41 & 1 \\
8109477 & mnist\_ddpm\_long & COMPLETED & 99 & 1 \\
8109577 & mnist\_ddpm\_long & COMPLETED & 411 & 1 \\
\bottomrule
\end{longtable}

Estimated direct GPU-hours from these three runs:
\[
\text{GPU-hours} = \frac{41 + 99 + 411}{3600} \times 1 = 0.153 \text{ GPU-hours (approx.)}
\]

\section{Diffusion Model Formulation}
The training script implements a DDPM-style denoising objective over MNIST ($x_0 \in [-1,1]^{1\times28\times28}$).

\subsection{Forward Diffusion}
Define variance schedule $\{\beta_t\}_{t=1}^T$ with
\[
\beta_t \in [10^{-4}, 2\times10^{-2}], \quad \alpha_t = 1-\beta_t, \quad \bar\alpha_t = \prod_{s=1}^{t} \alpha_s.
\]
The forward Markov transition is
\[
q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t}x_{t-1}, \beta_t I),
\]
with closed form sample
\[
x_t = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon, \quad \epsilon \sim \mathcal{N}(0,I).
\]

\subsection{Reverse Model and Objective}
The denoiser network $\epsilon_\theta(x_t,t)$ predicts injected noise. Reverse mean step used in sampling:
\[
\mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(x_t,t)\right).
\]
At training time, the objective is standard noise-prediction MSE:
\[
\mathcal{L}(\theta)=\mathbb{E}_{x_0,\epsilon,t}\left[\left\|\epsilon - \epsilon_\theta\big(\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon, t\big)\right\|_2^2\right].
\]

\section{Network Architecture}
The current model is a residual U-Net style denoiser with sinusoidal time embeddings:
\begin{itemize}
  \item Stem: Conv $1\rightarrow64$
  \item Down path: residual blocks at 64 and 128 channels with strided downsampling ($28\rightarrow14\rightarrow7$)
  \item Bottleneck: two residual blocks at 256 channels
  \item Up path: transposed convolutions + skip concatenation + residual blocks ($7\rightarrow14\rightarrow28$)
  \item Output: GroupNorm + Conv $64\rightarrow1$
  \item Optimizer: AdamW, gradient clipping (max norm 1.0)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.98\textwidth]{report/assets/gautschi_long_8109577/architecture_schematic.png}
\caption{U-Net denoiser schematic generated by the training pipeline.}
\end{figure}

\section{Latest Long Run Configuration}
The latest long run (job \texttt{8109577}) used:
\begin{itemize}
  \item Epochs: 80
  \item Diffusion steps ($T$): 300
  \item Batch size: 128
  \item Slurm resources: 1 H100 GPU, 14 CPUs, partition \texttt{ai}, walltime 4h
\end{itemize}

From \texttt{metrics.json} (run \texttt{long\_8109577}):
\begin{itemize}
  \item Total steps: 37,520
  \item Final epoch mean loss: 0.0362516
  \item Best epoch mean loss: 0.0360608
  \item Device: CUDA
\end{itemize}

\section{Results and Artifacts}
\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{report/assets/gautschi_long_8109577/mnist_samples.png}
\caption{Generated MNIST samples from the latest completed run.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_long_8109577/loss_curve_step.png}
\caption{Per-step training loss trajectory.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.88\textwidth]{report/assets/gautschi_long_8109577/loss_curve_epoch.png}
\caption{Per-epoch mean loss trajectory.}
\end{figure}

\section{Posterior Sampling with Partial Observations}
We added a post-training posterior sampling routine to \texttt{mnist\_diffusion.py} that performs likelihood-guided reverse diffusion under partial pixel observations. The latest implementation includes:
\begin{itemize}
  \item Noise-aware likelihood variance: $\sigma_t^2 = \sigma_y^2 + c(1-\bar{\alpha}_t)$
  \item Annealed guidance schedule: weak guidance at high noise, stronger guidance near final denoise steps
  \item Data-consistency projection on observed pixels (in both $\hat{x}_0$ and sampled states)
\end{itemize}

For this experiment:
\begin{itemize}
  \item Observed fraction: 70\% of pixels (30\% occluded, random mask)
  \item Target digit class: 7
  \item Guidance scale: 1.5
  \item Guidance min fraction / power: 0.25 / 1.5
  \item Likelihood noise scale: 0.1
  \item Noise-aware coefficient: 0.05
\end{itemize}

Validated Gautschi posterior job:
\begin{itemize}
  \item Job ID: \texttt{8112324}
  \item State: \texttt{COMPLETED}, exit code \texttt{0:0}
  \item Elapsed: \texttt{00:00:27}
  \item Run tag: \texttt{posterior\_70pct\_dpsplus\_8112324}
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{report/assets/gautschi_posterior_70pct_dpsplus/posterior_conditioning_overview.png}
\caption{Improved DPS conditioning setup: ground truth, observed mask (70\% observed), observed pixels, and posterior draws.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{report/assets/gautschi_posterior_70pct_dpsplus/posterior_samples.png}
\caption{Posterior samples with noise-aware likelihood, guidance annealing, and projection-based data consistency.}
\end{figure}

\section{Live Dashboard Operation}
The workflow now maintains a root live dashboard at:
\begin{itemize}
  \item \texttt{outputs/dashboard.html}
\end{itemize}
This dashboard follows the latest run via:
\begin{itemize}
  \item \texttt{outputs/LATEST\_RUN.txt}
  \item \texttt{outputs/current} symlink
\end{itemize}
and supports:
\begin{itemize}
  \item periodic refresh during active runs,
  \item automatic stop of refresh when status reaches \texttt{completed},
  \item image zoom controls for plots/schematics.
\end{itemize}

\section{Reproducibility Notes}
Primary scripts tracked in Git:
\begin{itemize}
  \item \texttt{mnist\_diffusion.py}
  \item \texttt{submit\_mnist\_diffusion\_long.slurm}
  \item \texttt{serve\_dashboard.sh}
\end{itemize}

To regenerate this report PDF locally:
\begin{verbatim}
cd PurdueHPC_Codex/report
latexmk -pdf -interaction=nonstopmode -halt-on-error \
  -output-directory=../output/pdf gautschi_diffusion_report.tex
\end{verbatim}

\end{document}
